{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9116a4",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 8\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For better aesthetics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.special import binom # Binomial coefficients for polynomial features\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efce25",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe46846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic data\n",
    "n = 60 \n",
    "beta0, beta1, sigma = 1.0, -0.8, 1.0 # Choose Some values for the true params\n",
    "x = np.linspace(-3, 3, n) # Clamp imput x to some range\n",
    "\n",
    "y =  # TODO: Implement data generating function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fit OLS model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot data, fitted line and vertical error (residual) bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do a histogram of the residuals (e.g. sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb699ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the log likelihood and the RSS as a function of beta1. Show visually that MLE and OLS are equivalent for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82792509",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "### Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c485f473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General hyperparameters\n",
    "TAU_SQ = 1.0       # Prior variance on weights\n",
    "SIGMA_SQ = 0.1**2  # Observation noise variance\n",
    "\n",
    "# Kernel-specific hyperparameters\n",
    "POLY_DEGREE = 9\n",
    "RBF_LENGTHSCALE = 0.1\n",
    "\n",
    "# Plotting settings\n",
    "NUM_SAMPLES = 5\n",
    "X_GRID = np.linspace(0, 1, 200).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99219de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_kernel(x1, x2, degree=POLY_DEGREE):\n",
    "    \"\"\"Computes the polynomial kernel k(x, x') = (1 + x*x')^degree.\"\"\"\n",
    "    # TODO: Implement the polynomial kernel function\n",
    "    pass\n",
    "\n",
    "def rbf_kernel(x1, x2, lengthscale=RBF_LENGTHSCALE):\n",
    "    \"\"\"Computes the RBF (squared-exponential) kernel.\"\"\"\n",
    "    # TODO: Implement the RBF kernel function\n",
    "    # Hint: You can use scipy.spatial.distance.cdist(x1, x2, 'sqeuclidean') to efficiently compute the squared distances\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5a8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_feature_map(x, degree=POLY_DEGREE):\n",
    "    \"\"\"Computes the feature map phi(x) for the polynomial kernel.\"\"\"\n",
    "    # TODO: Implement the feature map for the polynomial kernel\n",
    "    # The d-th feature is sqrt(C(degree, d)) * x^d\n",
    "    # Hint: A loop over the degree d from 0 to 'degree' is a good approach.\n",
    "    pass\n",
    "\n",
    "def sample_from_prior(kernel_func, **kwargs):\n",
    "    \"\"\"Samples functions from a GP prior defined by a kernel.\"\"\"\n",
    "    if kernel_func == polynomial_kernel:\n",
    "        # TODO: Implement prior sampling for the Polynomial kernel (weight-space view)\n",
    "        pass\n",
    "    else:\n",
    "        # TODO: Implement prior sampling for the RBF kernel (function-space view)\n",
    "        # Add a small 'jitter' (e.g., 1e-6 * identity matrix) to K for numerical stability\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa789df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_poly = sample_from_prior(polynomial_kernel)\n",
    "\n",
    "# Setup the 1x2 plot grid\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "fig.suptitle(\"Task 2.5(a): Visualizing Priors over Functions\", fontsize=16)\n",
    "\n",
    "# Polynomial Kernel\n",
    "axes[0].set_title(\"Prior Samples: Polynomial Kernel\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"f(x)\")\n",
    "axes[0].set_ylim(-3, 3) # Set common y-limit for easier comparison\n",
    "\n",
    "# TODO: Call your `sample_from_prior` function for the polynomial kernel\n",
    "# and plot the resulting function samples on axes[0].\n",
    "\n",
    "# RBF Kernel\n",
    "axes[1].set_title(f\"Prior Samples: RBF Kernel (l={RBF_LENGTHSCALE})\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"f(x)\")\n",
    "axes[1].set_ylim(-3, 3) \n",
    "\n",
    "# TODO: Call your `sample_from_prior` function for the RBF kernel\n",
    "# and plot the resulting function samples on axes[1].\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a11c46",
   "metadata": {},
   "source": [
    "**TODO**: Briefly comment on qualitative differences between the different kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e4a560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_predictive(X_train, y_train, kernel_func):\n",
    "    \"\"\"Computes the mean and variance of the posterior predictive distribution.\"\"\"\n",
    "    # TODO: Compute the required kernel matrices:\n",
    "    # K = k(X_train, X_train)\n",
    "    # K_star = k(X_GRID, X_train)\n",
    "    # K_star_star = k(X_GRID, X_GRID)\n",
    "    K = None\n",
    "    K_star = None\n",
    "    K_star_star = None\n",
    "    \n",
    "    # TODO: Compute the predictive mean using the kernel regression formula on the sheet\n",
    "    predictive_mean = None\n",
    "    \n",
    "    # TODO: Compute the epistemic covariance matrix using the formula from the sheet\n",
    "    epistemic_cov = None\n",
    "    \n",
    "    # Get the point-wise epistemic variance from the diagonal of the covariance matrix\n",
    "    epistemic_var = np.diag(epistemic_cov)\n",
    "    # --> Total predictive variance is the sum of epistemic and aleatoric (noise) variance\n",
    "    total_var = epistemic_var + SIGMA_SQ\n",
    "    \n",
    "    return predictive_mean, total_var, epistemic_var, epistemic_cov\n",
    "\n",
    "def sample_from_posterior(mean, cov):\n",
    "    \"\"\"Samples functions from the posterior predictive distribution.\"\"\"\n",
    "    # TODO: Draw NUM_SAMPLES from the multivariate normal distribution\n",
    "    # defined by the predictive mean and the epistemic covariance matrix\n",
    "    # Hint: Add a small jitter to 'cov' before sampling to ensure it is positive definite.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c86b6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "    return np.sin(2 * np.pi * x) + 0.5 * np.sin(4 * np.pi * x)\n",
    "\n",
    "def generate_data_with_gap(n=20, noise_std=np.sqrt(SIGMA_SQ)):\n",
    "    \"\"\"Generates data with a gap in the middle.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    x1 = np.random.uniform(0.0, 0.4, n // 2)\n",
    "    x2 = np.random.uniform(0.6, 1.0, n // 2)\n",
    "    X_train = np.concatenate([x1, x2]).reshape(-1, 1)\n",
    "    y_train = true_function(X_train.flatten()) + np.random.normal(0, noise_std, n)\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33edf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = # TODO: Generate data with gap\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n",
    "fig.suptitle(\" Posterior Distributions\", fontsize=16)\n",
    "\n",
    "# A dict to cleanly loop over the two kernel models\n",
    "kernels_to_test = {\n",
    "    \"Polynomial\": (polynomial_kernel, {'degree': POLY_DEGREE}),\n",
    "    \"RBF\": (rbf_kernel, {'lengthscale': RBF_LENGTHSCALE})\n",
    "}\n",
    "\n",
    "# 3. Loop through each kernel, compute its posterior, and plot\n",
    "for ax, (name, (kernel_func, kwargs)) in zip(axes, kernels_to_test.items()):\n",
    "    \n",
    "    # TODO: Call your functions to get the posterior predictive distribution\n",
    "    # and to draw samples from the posterior.\n",
    "    \n",
    "    # Calculate standard deviations for plotting 95% Gaussian credible intervals\n",
    "    total_std = np.sqrt(total_var)\n",
    "    epistemic_std = np.sqrt(epistemic_var)\n",
    "    \n",
    "    # Plot true function and training data\n",
    "    ax.plot(X_GRID, true_function(X_GRID.flatten()), 'k--', label=\"True Function\")\n",
    "    ax.plot(X_train, y_train, 'ko', label=\"Training Data\")\n",
    "    \n",
    "    # Plot predictive mean and credible intervals\n",
    "    ax.plot(X_GRID, mean, 'b-', lw=2, label=\"Predictive Mean\")\n",
    "    ax.fill_between(X_GRID.flatten(), mean - 1.96 * total_std, mean + 1.96 * total_std,\n",
    "                    color='gray', alpha=0.3, label=\"Total Uncertainty\")\n",
    "    ax.fill_between(X_GRID.flatten(), mean - 1.96 * epistemic_std, mean + 1.96 * epistemic_std,\n",
    "                    color='orange', alpha=0.5, label=\"Epistemic Uncertainty\")\n",
    "    \n",
    "    # Plot posterior function samples\n",
    "    ax.plot(X_GRID, posterior_samples, 'c-', alpha=0.4)\n",
    "    \n",
    "    # Final plot settings\n",
    "    ax.set_title(f\"Posterior: {name} Kernel\")\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40279d5b",
   "metadata": {},
   "source": [
    "**TODO**: Discuss the results.\n",
    "\n",
    "- Which kernel provides a more reasonable fit to the data and why?\n",
    "- Compare the epistemic uncertainty for both models. Where is it largest? How does it behave inside the data gap you created?\n",
    "- How do the posterior function samples relate to the uncertainty bands? Explain what the spread of these samples represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863b64b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
