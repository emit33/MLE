{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fc2510",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 4\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac18b4",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e48c7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5b53a-0f27-4352-8ff1-774104324ae8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1271b425",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the needed helper functions\n",
    "def tanh_prime(x_activated):\n",
    "    return \n",
    "\n",
    "def sigmoid(x):\n",
    "    return \n",
    "\n",
    "def binary_cross_entropy(y_hat, y):\n",
    "    \"\"\"\n",
    "    Computes the BCE loss over samples.\n",
    "    \"\"\"\n",
    "    # Hint: Add a small epsilon to y_hat to prevent numerical issues w/ log(0) issues (that's common practice in these cases)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9bc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Class\n",
    "class MLP:\n",
    "    def __init__(self, layer_dims, initialization_scale=1):\n",
    "        \"\"\"\n",
    "        Initializes the multi-layer perceptron.\n",
    "        Args:\n",
    "            layer_dims (list of int): List containing the number of neurons in each layer.\n",
    "                                      [d0, d1, d2] = [2, 10, 1] for the exercise.\n",
    "                                      d0: input dimension\n",
    "                                      d1: hidden layer dimension\n",
    "                                      d2: output dimension\n",
    "            initialization_scale (float): Scaling factor for weight initialization (i.e. standard deviation of the normal distribution)\n",
    "        \"\"\"\n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dims) \n",
    "\n",
    "        # Weights are initialized by drawing from a standard normal distribution, biases are initialized as zero.\n",
    "        # For more complex networks, one usually uses techniques like Xavier or He initialization. Play around with the initialization_scale parameter to see how it affects the training!\n",
    "        # Layer 0 -> Layer 1\n",
    "        self.parameters['W0'] = np.random.randn(layer_dims[1], layer_dims[0]) * initialization_scale \n",
    "        self.parameters['b0'] = np.zeros((layer_dims[1], 1))\n",
    "        # Layer 1 -> Layer 2\n",
    "        self.parameters['W1'] = np.random.randn(layer_dims[2], layer_dims[1]) * initialization_scale\n",
    "        self.parameters['b1'] = np.zeros((layer_dims[2], 1))\n",
    "        \n",
    "        self.cache = {} # For storing intermediate values (needed for backprop)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the network. Store intermediate values in self.cache for backward pass.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass & store the needed values in self.cache\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, Y_batch):\n",
    "        \"\"\"\n",
    "        Performs the backward pass (= backpropagation) to compute gradients of the loss with respect to the parameters.\n",
    "        Gradients are stored in the grads dictionary (see update_params method).\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward pass\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using gradient descent.\n",
    "        Args:\n",
    "            grads (dict): Dictionary of gradients.\n",
    "            learning_rate (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.parameters['W0'] -= learning_rate * grads['dW0']\n",
    "        self.parameters['b0'] -= learning_rate * grads['db0']\n",
    "        self.parameters['W1'] -= learning_rate * grads['dW1']\n",
    "        self.parameters['b1'] -= learning_rate * grads['db1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da95036",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "TODO: Explain why using vectorized operations is generally preferred in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a447f",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd7f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading and preprocessing (predefined)\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# NOTE: Different libraries/languages use different conventions for the shape of the data matrix X, which comes from the way they store data in memory:\n",
    "#   - In ML textbooks/mathematical notation, X is often (n_features, n_samples), because each sample is a (n_features, 1) column vector and we stack them \"horizontally\".\n",
    "#   - This is consistent with some languages (e.g. Julia, Matlab), which store data in column-major order.\n",
    "#   - However, most ML code (e.g. NumPy, sklearn, Pytorch) is optimized for row-major order, so you will in code most often see data matrices of shape (n_samples, n_features).\n",
    "#     (The reason for that is that most of these libraries run C/C++/CUDA code under the hood, which is optimized for row-major order)\n",
    "# --> Juggling the shapes of arrays to be correctly aligned with the used model implementation / convention is a day-to-day task in practical ML and the cause of many bugs, so always double-check the expected format.\n",
    "\n",
    "# TODO: The MLP class you're using expects its inputs in a specific shape, make sure your X and y match that convention.\n",
    "print(f\"Shape of X_train: {X_train.shape}\") \n",
    "print(f\"Shape of y_train: {y_train.shape}\") \n",
    "print(f\"Shape of X_test: {X_test.shape}\") \n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1e1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "layer_dimensions = [X_train.shape[0], 10, 1] # d0, d1, d2 as given in the exercise\n",
    "mlp = MLP(layer_dimensions) # Initialize the MLP\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = # TODO: Experiment with this\n",
    "num_epochs = # TODO: Experiment with this\n",
    "print_loss = # To monitor the training process, print the loss every few epochs\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    # Forward pass\n",
    "    y_hat_train = mlp.forward(X_train)\n",
    "    # Compute loss\n",
    "    train_loss = binary_cross_entropy(y_hat_train, y_train)\n",
    "    # Backward pass = backprop\n",
    "    grads = mlp.backward(y_train)\n",
    "   # Update parameters by gradient descent\n",
    "    mlp.update_params(grads, learning_rate)\n",
    "    \n",
    "    if epoch % print_loss == 0 or epoch == num_epochs:\n",
    "       train_losses.append(train_loss)\n",
    "       print(f\"Epoch {epoch}/{num_epochs} - Training Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "# TODO: Compute the accuracy on the test set and plot the decision boundary over the test set, comment on the performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
