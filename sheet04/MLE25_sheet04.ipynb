{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fc2510",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 4\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac18b4",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e48c7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5b53a-0f27-4352-8ff1-774104324ae8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 1\n",
    "\n",
    "a)\n",
    "$$\\frac{d}{dx}\\tanh(x) = \\frac{d}{dx} \\frac{\\sinh(x)}{\\cosh(x)} = 1 - \\tanh(x)^2 = \\frac{\\cosh(x)^2-\\sinh(x)^2}{\\cosh(x)^2} = \\frac{1}{\\cosh(x)^2}$$\n",
    "b)\n",
    "$$\\mathcal{L}(y,\\hat{y}) = -[y \\ln(\\hat{y})+ (1-y) \\ln(1-\\hat{y})] \\quad \\Rightarrow \\quad \\mathcal{\\delta}^{(2)} =\\frac{\\partial \\mathcal{L}}{\\partial\\tilde{z}^{(2)}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}}\\frac{\\partial z^{(2)}}{\\partial\\tilde{z}^{(2)}}$$\n",
    "with \n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}} = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})} \\quad \\mathrm{and} \\quad \\frac{\\partial z^{(2)}}{\\partial\\tilde{z}^{(2)}} = \\partial_{\\tilde{z}^{(2)}} \\sigma(\\tilde{z}^{(2)}) = \\sigma(\\tilde{z}^{(2)})\\sigma(-\\tilde{z}^{(2)}) = \\hat{y}(1-\\hat{y})$$\n",
    "and thus\n",
    "$$\\delta^{(2)}=\\hat{y}-y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271b425",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ce60ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the needed helper functions\n",
    "def tanh_prime(x_activated):\n",
    "    return 1 / (np.cosh(x_activated) ** 2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_hat, y):\n",
    "    \"\"\"\n",
    "    Computes the BCE loss over samples.\n",
    "    \"\"\"\n",
    "    # Hint: Add a small epsilon to y_hat to prevent numerical issues w/ log(0) issues (that's common practice in these cases)\n",
    "    epsilon = 1e-3\n",
    "    y_hat += epsilon\n",
    "    return np.mean(-(y*np.log(y_hat.flatten())+(1-y)*np.log(1-y_hat.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7c9bc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Class\n",
    "class MLP:\n",
    "    def __init__(self, layer_dims, initialization_scale=1):\n",
    "        \"\"\"\n",
    "        Initializes the multi-layer perceptron.\n",
    "        Args:\n",
    "            layer_dims (list of int): List containing the number of neurons in each layer.\n",
    "                                      [d0, d1, d2] = [2, 10, 1] for the exercise.\n",
    "                                      d0: input dimension\n",
    "                                      d1: hidden layer dimension\n",
    "                                      d2: output dimension\n",
    "            initialization_scale (float): Scaling factor for weight initialization (i.e. standard deviation of the normal distribution)\n",
    "        \"\"\"\n",
    "        self.parameters = {} #initialise dictionary\n",
    "        self.num_layers = len(layer_dims) \n",
    "\n",
    "        # Weights are initialized by drawing from a standard normal distribution, biases are initialized as zero.\n",
    "        # For more complex networks, one usually uses techniques like Xavier or He initialization. Play around with the initialization_scale parameter to see how it affects the training!\n",
    "        # Layer 0 -> Layer 1\n",
    "        self.parameters['W0'] = np.random.randn(layer_dims[1], layer_dims[0]) * initialization_scale \n",
    "        self.parameters['b0'] = np.zeros((layer_dims[1], 1))\n",
    "        # Layer 1 -> Layer 2\n",
    "        self.parameters['W1'] = np.random.randn(layer_dims[2], layer_dims[1]) * initialization_scale\n",
    "        self.parameters['b1'] = np.zeros((layer_dims[2], 1))\n",
    "        \n",
    "        self.cache = {} # For storing intermediate values (needed for backprop)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the network. Store intermediate values in self.cache for backward pass.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass & store the needed values in self.cache\n",
    "        z_1_preact = X_batch @ self.parameters['W0'].T + self.parameters['b0'].T\n",
    "        z_1 = np.tanh(z_1_preact)\n",
    "        y_hat = sigmoid(z_1 @ self.parameters['W1'].T + self.parameters['b1'].T)\n",
    "        self.cache['z0'] = X_batch\n",
    "        self.cache['z1_preact'] = z_1_preact\n",
    "        self.cache['z1'] = z_1\n",
    "        self.cache['y_hat'] = y_hat\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, Y_batch):\n",
    "        \"\"\"\n",
    "        Performs the backward pass (= backpropagation) to compute gradients of the loss with respect to the parameters.\n",
    "        Gradients are stored in the grads dictionary (see update_params method).\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward pass\n",
    "        grads = {}\n",
    "        del_2 = self.cache['y_hat'].T - Y_batch\n",
    "        grads['dW1'] = np.mean(self.cache['z1'].T * del_2,axis=1)\n",
    "        grads['db1'] = np.mean(del_2,axis=1) \n",
    "        del_1 = (self.parameters['W1'].T @ del_2)  * tanh_prime(self.cache['z1_preact']).T \n",
    "        grads['dW0'] = np.mean(del_1[np.newaxis,:] * self.cache['z0'].T[:,np.newaxis],axis=2) \n",
    "        grads['db0'] = np.mean(del_1,axis=1)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using gradient descent.\n",
    "        Args:\n",
    "            grads (dict): Dictionary of gradients.\n",
    "            learning_rate (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.parameters['W0'] -= learning_rate * grads['dW0'].T\n",
    "        self.parameters['b0'] -= learning_rate * grads['db0'][:,np.newaxis]\n",
    "        self.parameters['W1'] -= learning_rate * grads['dW1'].T\n",
    "        self.parameters['b1'] -= learning_rate * grads['db1'].T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da95036",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "TODO: Explain why using vectorized operations is generally preferred in ML.\n",
    "\n",
    "Using the built-in vectorized operations of numpy looping is performed in C instead of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a447f",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5afd7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (350, 2)\n",
      "Shape of y_train: (350,)\n",
      "Shape of X_test: (150, 2)\n",
      "Shape of y_test: (150,)\n"
     ]
    }
   ],
   "source": [
    "#Data loading and preprocessing (predefined)\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# NOTE: Different libraries/languages use different conventions for the shape of the data matrix X, which comes from the way they store data in memory:\n",
    "#   - In ML textbooks/mathematical notation, X is often (n_features, n_samples), because each sample is a (n_features, 1) column vector and we stack them \"horizontally\".\n",
    "#   - This is consistent with some languages (e.g. Julia, Matlab), which store data in column-major order.\n",
    "#   - However, most ML code (e.g. NumPy, sklearn, Pytorch) is optimized for row-major order, so you will in code most often see data matrices of shape (n_samples, n_features).\n",
    "#     (The reason for that is that most of these libraries run C/C++/CUDA code under the hood, which is optimized for row-major order)\n",
    "# --> Juggling the shapes of arrays to be correctly aligned with the used model implementation / convention is a day-to-day task in practical ML and the cause of many bugs, so always double-check the expected format.\n",
    "\n",
    "# TODO: The MLP class you're using expects its inputs in a specific shape, make sure your X and y match that convention.\n",
    "print(f\"Shape of X_train: {X_train.shape}\") \n",
    "print(f\"Shape of y_train: {y_train.shape}\") \n",
    "print(f\"Shape of X_test: {X_test.shape}\") \n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1f1e1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39087244147452865\n",
      "Epoch 10/1000 - Training Loss: 0.3909\n",
      "0.32659116268445254\n",
      "Epoch 20/1000 - Training Loss: 0.3266\n",
      "0.3044045601252755\n",
      "Epoch 30/1000 - Training Loss: 0.3044\n",
      "0.29194692313106907\n",
      "Epoch 40/1000 - Training Loss: 0.2919\n",
      "0.28347772218436573\n",
      "Epoch 50/1000 - Training Loss: 0.2835\n",
      "0.27718601454928476\n",
      "Epoch 60/1000 - Training Loss: 0.2772\n",
      "0.27225756512982513\n",
      "Epoch 70/1000 - Training Loss: 0.2723\n",
      "0.26824054105862377\n",
      "Epoch 80/1000 - Training Loss: 0.2682\n",
      "0.2648553958476801\n",
      "Epoch 90/1000 - Training Loss: 0.2649\n",
      "0.2619196174016594\n",
      "Epoch 100/1000 - Training Loss: 0.2619\n",
      "0.25931049990104976\n",
      "Epoch 110/1000 - Training Loss: 0.2593\n",
      "0.25694391157110313\n",
      "Epoch 120/1000 - Training Loss: 0.2569\n",
      "0.25476123004327333\n",
      "Epoch 130/1000 - Training Loss: 0.2548\n",
      "0.2527209831918728\n",
      "Epoch 140/1000 - Training Loss: 0.2527\n",
      "0.25079337982853567\n",
      "Epoch 150/1000 - Training Loss: 0.2508\n",
      "0.24895667622545656\n",
      "Epoch 160/1000 - Training Loss: 0.2490\n",
      "0.2471947332507159\n",
      "Epoch 170/1000 - Training Loss: 0.2472\n",
      "0.2454953575411315\n",
      "Epoch 180/1000 - Training Loss: 0.2455\n",
      "0.2438491655336429\n",
      "Epoch 190/1000 - Training Loss: 0.2438\n",
      "0.24224879992617368\n",
      "Epoch 200/1000 - Training Loss: 0.2422\n",
      "0.24068838578269233\n",
      "Epoch 210/1000 - Training Loss: 0.2407\n",
      "0.23916315069400695\n",
      "Epoch 220/1000 - Training Loss: 0.2392\n",
      "0.23766915778079603\n",
      "Epoch 230/1000 - Training Loss: 0.2377\n",
      "0.2362031165380673\n",
      "Epoch 240/1000 - Training Loss: 0.2362\n",
      "0.23476224745322433\n",
      "Epoch 250/1000 - Training Loss: 0.2348\n",
      "0.23334418379033992\n",
      "Epoch 260/1000 - Training Loss: 0.2333\n",
      "0.2319468990715346\n",
      "Epoch 270/1000 - Training Loss: 0.2319\n",
      "0.23056865234770588\n",
      "Epoch 280/1000 - Training Loss: 0.2306\n",
      "0.22920794582694595\n",
      "Epoch 290/1000 - Training Loss: 0.2292\n",
      "0.22786349115041277\n",
      "Epoch 300/1000 - Training Loss: 0.2279\n",
      "0.226534181798389\n",
      "Epoch 310/1000 - Training Loss: 0.2265\n",
      "0.22521906993092533\n",
      "Epoch 320/1000 - Training Loss: 0.2252\n",
      "0.22391734652831213\n",
      "Epoch 330/1000 - Training Loss: 0.2239\n",
      "0.22262832407494304\n",
      "Epoch 340/1000 - Training Loss: 0.2226\n",
      "0.22135142128163157\n",
      "Epoch 350/1000 - Training Loss: 0.2214\n",
      "0.22008614950573793\n",
      "Epoch 360/1000 - Training Loss: 0.2201\n",
      "0.21883210063363215\n",
      "Epoch 370/1000 - Training Loss: 0.2188\n",
      "0.21758893625581888\n",
      "Epoch 380/1000 - Training Loss: 0.2176\n",
      "0.2163563780052156\n",
      "Epoch 390/1000 - Training Loss: 0.2164\n",
      "0.21513419895304287\n",
      "Epoch 400/1000 - Training Loss: 0.2151\n",
      "0.21392221597089814\n",
      "Epoch 410/1000 - Training Loss: 0.2139\n",
      "0.21272028297601867\n",
      "Epoch 420/1000 - Training Loss: 0.2127\n",
      "0.21152828498215043\n",
      "Epoch 430/1000 - Training Loss: 0.2115\n",
      "0.2103461328824685\n",
      "Epoch 440/1000 - Training Loss: 0.2103\n",
      "0.2091737588945519\n",
      "Epoch 450/1000 - Training Loss: 0.2092\n",
      "0.20801111260101285\n",
      "Epoch 460/1000 - Training Loss: 0.2080\n",
      "0.2068581575231986\n",
      "Epoch 470/1000 - Training Loss: 0.2069\n",
      "0.20571486816947374\n",
      "Epoch 480/1000 - Training Loss: 0.2057\n",
      "0.20458122750389698\n",
      "Epoch 490/1000 - Training Loss: 0.2046\n",
      "0.20345722478553505\n",
      "Epoch 500/1000 - Training Loss: 0.2035\n",
      "0.20234285373310065\n",
      "Epoch 510/1000 - Training Loss: 0.2023\n",
      "0.20123811097397798\n",
      "Epoch 520/1000 - Training Loss: 0.2012\n",
      "0.20014299474090613\n",
      "Epoch 530/1000 - Training Loss: 0.2001\n",
      "0.19905750378358666\n",
      "Epoch 540/1000 - Training Loss: 0.1991\n",
      "0.19798163646620678\n",
      "Epoch 550/1000 - Training Loss: 0.1980\n",
      "0.19691539002530542\n",
      "Epoch 560/1000 - Training Loss: 0.1969\n",
      "0.19585875996554414\n",
      "Epoch 570/1000 - Training Loss: 0.1959\n",
      "0.19481173957377088\n",
      "Epoch 580/1000 - Training Loss: 0.1948\n",
      "0.19377431953430022\n",
      "Epoch 590/1000 - Training Loss: 0.1938\n",
      "0.192746487630582\n",
      "Epoch 600/1000 - Training Loss: 0.1927\n",
      "0.1917282285204227\n",
      "Epoch 610/1000 - Training Loss: 0.1917\n",
      "0.19071952357366712\n",
      "Epoch 620/1000 - Training Loss: 0.1907\n",
      "0.18972035076277766\n",
      "Epoch 630/1000 - Training Loss: 0.1897\n",
      "0.18873068459807785\n",
      "Epoch 640/1000 - Training Loss: 0.1887\n",
      "0.18775049610058026\n",
      "Epoch 650/1000 - Training Loss: 0.1878\n",
      "0.18677975280631728\n",
      "Epoch 660/1000 - Training Loss: 0.1868\n",
      "0.1858184187969569\n",
      "Epoch 670/1000 - Training Loss: 0.1858\n",
      "0.18486645475222582\n",
      "Epoch 680/1000 - Training Loss: 0.1849\n",
      "0.18392381802030391\n",
      "Epoch 690/1000 - Training Loss: 0.1839\n",
      "0.18299046270290087\n",
      "Epoch 700/1000 - Training Loss: 0.1830\n",
      "0.18206633975219916\n",
      "Epoch 710/1000 - Training Loss: 0.1821\n",
      "0.18115139707725314\n",
      "Epoch 720/1000 - Training Loss: 0.1812\n",
      "0.18024557965777976\n",
      "Epoch 730/1000 - Training Loss: 0.1802\n",
      "0.17934882966357787\n",
      "Epoch 740/1000 - Training Loss: 0.1793\n",
      "0.17846108657806795\n",
      "Epoch 750/1000 - Training Loss: 0.1785\n",
      "0.1775822873246665\n",
      "Epoch 760/1000 - Training Loss: 0.1776\n",
      "0.1767123663948981\n",
      "Epoch 770/1000 - Training Loss: 0.1767\n",
      "0.17585125597731338\n",
      "Epoch 780/1000 - Training Loss: 0.1759\n",
      "0.1749988860864211\n",
      "Epoch 790/1000 - Training Loss: 0.1750\n",
      "0.17415518469096583\n",
      "Epoch 800/1000 - Training Loss: 0.1742\n",
      "0.17332007784098702\n",
      "Epoch 810/1000 - Training Loss: 0.1733\n",
      "0.172493489793188\n",
      "Epoch 820/1000 - Training Loss: 0.1725\n",
      "0.1716753431342204\n",
      "Epoch 830/1000 - Training Loss: 0.1717\n",
      "0.17086555890155994\n",
      "Epoch 840/1000 - Training Loss: 0.1709\n",
      "0.17006405670170877\n",
      "Epoch 850/1000 - Training Loss: 0.1701\n",
      "0.1692707548255087\n",
      "Epoch 860/1000 - Training Loss: 0.1693\n",
      "0.16848557036039835\n",
      "Epoch 870/1000 - Training Loss: 0.1685\n",
      "0.16770841929948246\n",
      "Epoch 880/1000 - Training Loss: 0.1677\n",
      "0.16693921664731695\n",
      "Epoch 890/1000 - Training Loss: 0.1669\n",
      "0.1661778765223428\n",
      "Epoch 900/1000 - Training Loss: 0.1662\n",
      "0.16542431225592555\n",
      "Epoch 910/1000 - Training Loss: 0.1654\n",
      "0.16467843648798008\n",
      "Epoch 920/1000 - Training Loss: 0.1647\n",
      "0.1639401612591789\n",
      "Epoch 930/1000 - Training Loss: 0.1639\n",
      "0.16320939809975799\n",
      "Epoch 940/1000 - Training Loss: 0.1632\n",
      "0.16248605811494785\n",
      "Epoch 950/1000 - Training Loss: 0.1625\n",
      "0.16177005206706926\n",
      "Epoch 960/1000 - Training Loss: 0.1618\n",
      "0.16106129045434334\n",
      "Epoch 970/1000 - Training Loss: 0.1611\n",
      "0.1603596835864728\n",
      "Epoch 980/1000 - Training Loss: 0.1604\n",
      "0.1596651416570597\n",
      "Epoch 990/1000 - Training Loss: 0.1597\n",
      "0.15897757481292837\n",
      "Epoch 1000/1000 - Training Loss: 0.1590\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "layer_dimensions = [X_train.shape[1], 10, 1] # d0, d1, d2 as given in the exercise\n",
    "mlp = MLP(layer_dimensions) # Initialize the MLP\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1# TODO: Experiment with this\n",
    "num_epochs = 1000# TODO: Experiment with this\n",
    "print_loss = 10# To monitor the training process, print the loss every few epochs\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    # Forward pass\n",
    "    y_hat_train = mlp.forward(X_train)\n",
    "    # Compute loss\n",
    "    train_loss = binary_cross_entropy(y_hat_train, y_train)\n",
    "    # Backward pass = backprop\n",
    "    grads = mlp.backward(y_train)\n",
    "   # Update parameters by gradient descent\n",
    "    mlp.update_params(grads, learning_rate)\n",
    "    \n",
    "    if epoch % print_loss == 0 or epoch == num_epochs:\n",
    "       train_losses.append(train_loss)\n",
    "       print(train_loss)\n",
    "       print(f\"Epoch {epoch}/{num_epochs} - Training Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992a7a5-8457-45c1-9b1c-473dfd4a5b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "# TODO: Compute the accuracy on the test set and plot the decision boundary over the test set, comment on the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e90ce3-4030-41a6-a918-78f23c054a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
