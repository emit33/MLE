{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fc2510",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 4\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeac18b4",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48c7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5b53a-0f27-4352-8ff1-774104324ae8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Task 1\n",
    "\n",
    "a)\n",
    "$$\\frac{d}{dx}\\tanh(x) = \\frac{d}{dx} \\frac{\\sinh(x)}{\\cosh(x)} = 1 - \\tanh(x)^2 = \\frac{\\cosh(x)^2-\\sinh(x)^2}{\\cosh(x)^2} = \\frac{1}{\\cosh(x)^2}$$\n",
    "b)\n",
    "$$\\mathcal{L}(y,\\hat{y}) = -[y \\ln(\\hat{y})+ (1-y) \\ln(1-\\hat{y})] \\quad \\Rightarrow \\quad \\mathcal{\\delta}^{(2)} =\\frac{\\partial \\mathcal{L}}{\\partial\\tilde{z}^{(2)}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}}\\frac{\\partial z^{(2)}}{\\partial\\tilde{z}^{(2)}}$$\n",
    "with \n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial z^{(2)}} = \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})} \\quad \\mathrm{and} \\quad \\frac{\\partial z^{(2)}}{\\partial\\tilde{z}^{(2)}} = \\partial_{\\tilde{z}^{(2)}} \\sigma(\\tilde{z}^{(2)}) = \\sigma(\\tilde{z}^{(2)})\\sigma(-\\tilde{z}^{(2)}) = \\hat{y}(1-\\hat{y})$$\n",
    "and thus\n",
    "$$\\delta^{(2)}=\\hat{y}-y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271b425",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce60ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define the needed helper functions\n",
    "def tanh_prime(x_activated):\n",
    "    return 1 / (np.cosh(x_activated) ** 2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_hat, y):\n",
    "    \"\"\"\n",
    "    Computes the BCE loss over samples.\n",
    "    \"\"\"\n",
    "    # Hint: Add a small epsilon to y_hat to prevent numerical issues w/ log(0) issues (that's common practice in these cases)\n",
    "    epsilon = 1e-3\n",
    "    y_hat += epsilon\n",
    "    return -(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7c9bc60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Class\n",
    "class MLP:\n",
    "    def __init__(self, layer_dims, initialization_scale=1):\n",
    "        \"\"\"\n",
    "        Initializes the multi-layer perceptron.\n",
    "        Args:\n",
    "            layer_dims (list of int): List containing the number of neurons in each layer.\n",
    "                                      [d0, d1, d2] = [2, 10, 1] for the exercise.\n",
    "                                      d0: input dimension\n",
    "                                      d1: hidden layer dimension\n",
    "                                      d2: output dimension\n",
    "            initialization_scale (float): Scaling factor for weight initialization (i.e. standard deviation of the normal distribution)\n",
    "        \"\"\"\n",
    "        self.parameters = {} #initialise dictionary\n",
    "        self.num_layers = len(layer_dims) \n",
    "\n",
    "        # Weights are initialized by drawing from a standard normal distribution, biases are initialized as zero.\n",
    "        # For more complex networks, one usually uses techniques like Xavier or He initialization. Play around with the initialization_scale parameter to see how it affects the training!\n",
    "        # Layer 0 -> Layer 1\n",
    "        self.parameters['W0'] = np.random.randn(layer_dims[1], layer_dims[0]) * initialization_scale \n",
    "        self.parameters['b0'] = np.zeros((layer_dims[1], 1))\n",
    "        # Layer 1 -> Layer 2\n",
    "        self.parameters['W1'] = np.random.randn(layer_dims[2], layer_dims[1]) * initialization_scale\n",
    "        self.parameters['b1'] = np.zeros((layer_dims[2], 1))\n",
    "        \n",
    "        self.cache = {} # For storing intermediate values (needed for backprop)\n",
    "\n",
    "    def forward(self, X_batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the network. Store intermediate values in self.cache for backward pass.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the forward pass & store the needed values in self.cache\n",
    "        z_1_preact = X_batch @ self.parameters['W0'].T + self.parameters['b0'].T\n",
    "        z_1 = np.tanh(z_1_preact)\n",
    "        y_hat = sigmoid(z_1 @ self.parameters['W1'].T + self.parameters['b1'].T)\n",
    "        self.cache['z0'] = X_batch\n",
    "        self.cache['z1_preact'] = z_1_preact\n",
    "        self.cache['z1'] = z_1\n",
    "        self.cache['y_hat'] = y_hat\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, Y_batch):\n",
    "        \"\"\"\n",
    "        Performs the backward pass (= backpropagation) to compute gradients of the loss with respect to the parameters.\n",
    "        Gradients are stored in the grads dictionary (see update_params method).\n",
    "        \"\"\"\n",
    "        # TODO: Implement the backward pass\n",
    "        grads = {}\n",
    "        del_2 = self.cache['y_hat'].T - Y_batch\n",
    "        grads['dW1'] = del_2 * self.cache['z1'].T\n",
    "        grads['db1'] = del_2 \n",
    "        print(del_2.shape,self.cache['z1'].T.shape)\n",
    "        del_1 = (self.parameters['W1'].T @ del_2)  * tanh_prime(self.cache['z1_preact']).T\n",
    "        grad['dW0'] = del_1 * self.cache['z0'].T * del_2        grad['db0'] = del_1\n",
    "        \n",
    "        return [grad_W0,grad_b0,grad_W1,grad_b1]\n",
    "\n",
    "    def update_params(self, grads, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters using gradient descent.\n",
    "        Args:\n",
    "            grads (dict): Dictionary of gradients.\n",
    "            learning_rate (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.parameters['W0'] -= learning_rate * grads['dW0']\n",
    "        self.parameters['b0'] -= learning_rate * grads['db0']\n",
    "        self.parameters['W1'] -= learning_rate * grads['dW1']\n",
    "        self.parameters['b1'] -= learning_rate * grads['db1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da95036",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "TODO: Explain why using vectorized operations is generally preferred in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a447f",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5afd7f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (350, 2)\n",
      "Shape of y_train: (350,)\n",
      "Shape of X_test: (150, 2)\n",
      "Shape of y_test: (150,)\n"
     ]
    }
   ],
   "source": [
    "#Data loading and preprocessing (predefined)\n",
    "X, y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# NOTE: Different libraries/languages use different conventions for the shape of the data matrix X, which comes from the way they store data in memory:\n",
    "#   - In ML textbooks/mathematical notation, X is often (n_features, n_samples), because each sample is a (n_features, 1) column vector and we stack them \"horizontally\".\n",
    "#   - This is consistent with some languages (e.g. Julia, Matlab), which store data in column-major order.\n",
    "#   - However, most ML code (e.g. NumPy, sklearn, Pytorch) is optimized for row-major order, so you will in code most often see data matrices of shape (n_samples, n_features).\n",
    "#     (The reason for that is that most of these libraries run C/C++/CUDA code under the hood, which is optimized for row-major order)\n",
    "# --> Juggling the shapes of arrays to be correctly aligned with the used model implementation / convention is a day-to-day task in practical ML and the cause of many bugs, so always double-check the expected format.\n",
    "\n",
    "# TODO: The MLP class you're using expects its inputs in a specific shape, make sure your X and y match that convention.\n",
    "print(f\"Shape of X_train: {X_train.shape}\") \n",
    "print(f\"Shape of y_train: {y_train.shape}\") \n",
    "print(f\"Shape of X_test: {X_test.shape}\") \n",
    "print(f\"Shape of y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "40f7cd68-039e-4a31-b434-da9c2fdae0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 2)\n",
      "(10, 2)\n",
      "(350, 10)\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [X_train.shape[1], 10, 1]\n",
    "matrix = np.random.randn(layer_dims[1], layer_dims[0])\n",
    "print(X_train.shape)\n",
    "print(matrix.shape)\n",
    "print((X_train @ matrix.T).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1f1e1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 350) (10, 350)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,350) (2,350) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m  train_loss = binary_cross_entropy(y_hat_train, y_train)\n\u001b[32m     16\u001b[39m  \u001b[38;5;66;03m# Backward pass = backprop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m  grads = \u001b[43mmlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Update parameters by gradient descent\u001b[39;00m\n\u001b[32m     19\u001b[39m  mlp.update_params(grads, learning_rate)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mMLP.backward\u001b[39m\u001b[34m(self, Y_batch)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(del_2.shape,\u001b[38;5;28mself\u001b[39m.cache[\u001b[33m'\u001b[39m\u001b[33mz1\u001b[39m\u001b[33m'\u001b[39m].T.shape)\n\u001b[32m     53\u001b[39m del_1 = (\u001b[38;5;28mself\u001b[39m.parameters[\u001b[33m'\u001b[39m\u001b[33mW1\u001b[39m\u001b[33m'\u001b[39m].T @ del_2)  * tanh_prime(\u001b[38;5;28mself\u001b[39m.cache[\u001b[33m'\u001b[39m\u001b[33mz1_preact\u001b[39m\u001b[33m'\u001b[39m]).T\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m grad[\u001b[33m'\u001b[39m\u001b[33mdW0\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdel_1\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mz0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\n\u001b[32m     55\u001b[39m grad[\u001b[33m'\u001b[39m\u001b[33mdb0\u001b[39m\u001b[33m'\u001b[39m] = del_1\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [grad_W0,grad_b0,grad_W1,grad_b1]\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (10,350) (2,350) "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "layer_dimensions = [X_train.shape[1], 10, 1] # d0, d1, d2 as given in the exercise\n",
    "mlp = MLP(layer_dimensions) # Initialize the MLP\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1# TODO: Experiment with this\n",
    "num_epochs = 10# TODO: Experiment with this\n",
    "print_loss = 2# To monitor the training process, print the loss every few epochs\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    # Forward pass\n",
    "    y_hat_train = mlp.forward(X_train)\n",
    "    # Compute loss\n",
    "    train_loss = binary_cross_entropy(y_hat_train, y_train)\n",
    "    # Backward pass = backprop\n",
    "    grads = mlp.backward(y_train)\n",
    "   # Update parameters by gradient descent\n",
    "    mlp.update_params(grads, learning_rate)\n",
    "    \n",
    "    if epoch % print_loss == 0 or epoch == num_epochs:\n",
    "       train_losses.append(train_loss)\n",
    "       print(f\"Epoch {epoch}/{num_epochs} - Training Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8992a7a5-8457-45c1-9b1c-473dfd4a5b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3493666087.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(self.cache['y_hat'].shape)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d3aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation \n",
    "# TODO: Compute the accuracy on the test set and plot the decision boundary over the test set, comment on the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e90ce3-4030-41a6-a918-78f23c054a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
