{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0310868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#silences the jupyter notebook output in the command line\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68b00e",
   "metadata": {},
   "source": [
    "\n",
    "test ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae27db5",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 1\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a0b426",
   "metadata": {},
   "source": [
    "## Exercise 2: The Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c408f",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748fe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Load & Visualize the Dataset\n",
    "# ===============================\n",
    "\n",
    "# TODO: Load dataset, print feature names\n",
    "\n",
    "\n",
    "# TODO: Select features & corresponding labels\n",
    "X = \n",
    "y = \n",
    "\n",
    "# Convert labels from {0,1} to {-1,1} to match Perceptron convention from sheet\n",
    "y = 2 * (y - 0.5)\n",
    "\n",
    "# TODO: Standardize the data to zero mean and unit variance, explain why it's useful\n",
    "\n",
    "# TODO: Visualize dataset using plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2. Implement the Perceptron's training algorithm\n",
    "# ========================\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.1, num_epochs=10):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.w = None  # Weights\n",
    "        self.b = None # Bias\n",
    "        self.history = [] # Store parameters for decision boundary @ each update for visualization\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the perceptron using the online Perceptron algorithm.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # TODO: Initialize weights and bias\n",
    "        self.w = \n",
    "        self.b =\n",
    "\n",
    "        # Train for num_epochs iterations\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(n_samples):\n",
    "                X_i = X[i]\n",
    "                # TODO: Implement the update rule\n",
    "                if y[i] != self.predict(X_i):\n",
    "                    self.w = \n",
    "                    self.b =\n",
    "                    self.history.append((self.w,self.b)) # Save state for visualization \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the label of a sample.\"\"\"\n",
    "        # TODO: Implement the prediction function\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f51a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 3. Train the Perceptron & Evaluate Performance\n",
    "# ========================\n",
    "\n",
    "#TODO: Split the data into training and test set\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "#TODO: Initialize the Perceptron and train it on the training set\n",
    "perceptron = \n",
    "...\n",
    "\n",
    "#TODO: Use the trained Perceptron to compute the accuracy on the training set and on the test set\n",
    "...\n",
    "train_acc =\n",
    "test_acc =\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.3f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 4. Plot decision boundary evolution\n",
    "# ========================\n",
    "\n",
    "# Visualize the first 5 consecutive decision boundaries for data\n",
    "decision_boundaries = perceptron.history[:5] # Get the parameters of the first 5 decision boundaries used during training\n",
    "\n",
    "# TODO: Plot decision boundaries for iterations 1-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3364aa",
   "metadata": {},
   "source": [
    "### 5.\n",
    "TODO: How many updates do you need until convergence (i.e. until no more model updates\n",
    "occur)? Explain why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46274cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 6. Evaluate Performance Over Multiple Runs\n",
    "# ========================\n",
    "\n",
    "#TODO: Evaluate performance over multiple runs. Compute and store test accuracies\n",
    "\n",
    "\n",
    "#TODO: Plot histogram for the test accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25899265",
   "metadata": {},
   "source": [
    "### (a)\n",
    "TODO: What does the shape of the histogram tell you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4681de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b)\n",
    "#TODO: Compute the sample mean and standard deviation of the test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33f11a",
   "metadata": {},
   "source": [
    "### (c)\n",
    "TODO: Given enough data points and for many training runs, what type of probability distribution would the histogram approximate and what is the reason for that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d7831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (d)\n",
    "p_values = [0, 10, 20, 30, 40, 50] # % of flipped training labels\n",
    "#TODO: Add noise by flipping p% of labels. Visualize the effect using histograms for each p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5592766",
   "metadata": {},
   "source": [
    "TODO: Interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fbb30",
   "metadata": {},
   "source": [
    "## Exercise 3: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb1c0460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles, make_blobs\n",
    "from cvxopt import matrix, solvers # Install cvxopt via \"pip install cvxopt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. Complete SVM implementation\n",
    "# ========================\n",
    "\n",
    "class DualSVM:\n",
    "    def __init__(self, C=1.0, kernel=\"linear\", gamma=1.0):\n",
    "        self.C = C # Regularization constant\n",
    "        self.kernel = kernel # Kernel type: \"linear\" or \"rbf\"\n",
    "        self.gamma = gamma # Kernel parameter (\"bandwith\")\n",
    "        self.alpha = None # Lagrange multipliers\n",
    "        self.sv_X = None # Support vectors\n",
    "        self.sv_y = None # Support vector labels\n",
    "        self.w = None # Weights\n",
    "        self.b = None # Bias\n",
    "\n",
    "    def linear_kernel(self, X1, X2):\n",
    "        #TODO: Implement linear kernel\n",
    "        return \n",
    "\n",
    "    def rbf_kernel(self, X1, X2):\n",
    "        #TODO: Implement RBF kernel \n",
    "        return \n",
    "\n",
    "    def compute_kernel(self, X1, X2):\n",
    "        if self.kernel == \"linear\":\n",
    "            return self.linear_kernel(X1, X2)\n",
    "        elif self.kernel == \"rbf\":\n",
    "            return self.rbf_kernel(X1, X2)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown kernel type.\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Compute kernel matrix K: K[i,j] = K(x_i, x_j)\n",
    "        K = self.compute_kernel(X, X)\n",
    "\n",
    "        \"\"\"\n",
    "        The dual objective is:\n",
    "            max  sum_i alpha_i - 1/2 sum_i sum_j alpha_i alpha_j y_i y_j K(x_i, x_j)\n",
    "        subject to:\n",
    "            sum_i alpha_i y_i = 0  and  0 <= alpha_i <= C for all i.\n",
    "        In QP formulation:\n",
    "            P = (y_i y_j K(x_i,x_j))_{i,j},   q = -1 (vector),\n",
    "            A = y^T, b = 0, and G, h implement 0 <= alpha_i <= C.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Use the matrix function of cvxopt to define the QP parameters\n",
    "        P = matrix()\n",
    "        q = matrix()\n",
    "        A = matrix(y, (1, n_samples),\"d\") # Use \"d\" flag to make sure that the matrix is in double precision format (labels are integers)\n",
    "        b = matrix()\n",
    "        \n",
    "        \n",
    "        # TODO: Implement inequality constraints by defining G and h\n",
    "        G = matrix()\n",
    "        h = matrix()\n",
    "\n",
    "        # Solve the QP problem using cvxopt\n",
    "        solvers.options[\"show_progress\"] = False\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "        alphas = np.ravel(solution[\"x\"]) # Get optimal alphas\n",
    "\n",
    "        # Get support vectors (i.e. data points with non-zero lagrange multipliers, that are on the margin)\n",
    "        sv = alphas > 1e-5 # alpha > 1e-5 to account for numerical errors\n",
    "        self.alpha = alphas[sv]\n",
    "        self.sv_X = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "\n",
    "        # The bias corresponds to the average error over all support vectors:\n",
    "        # Why does the bias corresponds to the average error over all support vectors?\n",
    "        # The answer is that the bias is the average of the differences between the true labels and the predicted labels\n",
    "        # for the support vectors. The predicted labels are computed by the decision function f(x) = sum(alpha_i y_i K(x,x_i)) + b.\n",
    "        # The difference between the true labels and the predicted labels is the error for each support vector.\n",
    "        # The bias is the average of these errors.\n",
    "        self.b = np.mean(self.sv_y - np.sum(self.alpha * self.sv_y * K[sv][:, sv], axis=1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        #TODO: Implement the decision function and return the corresponding predicted labels\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53972b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2. Apply linear SVM on blobs\n",
    "# ========================\n",
    "\n",
    "# TODO: Generate blobs dataset\n",
    "X_linear, y_linear = \n",
    "\n",
    "# Convert labels from {0,1} to {-1,1}\n",
    "y_linear = 2 * (y_linear - 0.5) \n",
    "\n",
    "#TODO: Train SVM with linear kernel\n",
    "\n",
    "#TODO: Plot decision boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b92e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 3. Apply linear SVM on circles\n",
    "# ===========================\n",
    "\n",
    "#TODO: Generate circles dataset\n",
    "X_circles, y_circles = \n",
    "y_circles = 2 * (y_circles - 0.5)  # Convert labels from {0,1} to {-1,1}\n",
    "\n",
    "#TODO: Train SVM with linear kernel\n",
    "\n",
    "#TODO: Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 4. Apply feature transformation \n",
    "# ===========================\n",
    "\n",
    "def transform_features(X):\n",
    "    # TODO: compute feature transformation: f(x) = [x1, x2, x1^2 + x2^2]\n",
    "    return \n",
    "\n",
    "\n",
    "#TODO: Train SVM with linear kernel on transformed features\n",
    "\n",
    "def plot_decision_boundary_transformed(X, y, model, title=\"SVM Decision Boundary (Transformed)\"):\n",
    "    # TODO: Implement plotting function for decision boundary in the transformed feature space\n",
    "    # Hint: You could do this by creating a 2D meshgrid which you transform using the feature mapping.\n",
    "    # Then, after evaluating the model on it, you can plot the result as a contour plot (plt.contourf).\n",
    "    ...\n",
    "    plt.show()\n",
    "\n",
    "#TODO: Plot decision boundary in the transformed feature space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ff899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5. SVM with RBF Kernel on Circular Data\n",
    "# ===========================\n",
    "\n",
    "#TODO: Train SVM with RBF kernel on circular data\n",
    "\n",
    "#TODO: Plot decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de0adcd",
   "metadata": {},
   "source": [
    "### 6.\n",
    "\n",
    "TODO: Compare the decision boundaries from Tasks 3, 4, and 5. How does feature transformation differ from using an RBF kernel? When would one approach be preferable to the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5e19ad",
   "metadata": {},
   "source": [
    "### 7. \n",
    "\n",
    "TODO: Besides the dual formulation, SVMs also have an equivalent primal formulation. The key factor in choosing which one to use as the optimization criterion is the dimensionality of the features. Explain why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
