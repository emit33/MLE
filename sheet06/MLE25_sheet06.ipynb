{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f627c6b1",
   "metadata": {},
   "source": [
    "# Machine Learning Essentials SS25 - Exercise Sheet 6\n",
    "\n",
    "## Instructions\n",
    "- `TODO`'s indicate where you need to complete the implementations.\n",
    "- You may use external resources, but <b>write your own solutions</b>.\n",
    "- Provide concise, but comprehensible comments to explain what your code does.\n",
    "- Code that's unnecessarily extensive and/or not well commented will not be scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as tc\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "np.random.seed(42)\n",
    "tc.manual_seed(42)\n",
    "\n",
    "device = tc.device(\"cuda\" if tc.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6106fd",
   "metadata": {},
   "source": [
    "## Exercise 2 - CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc7c72",
   "metadata": {},
   "source": [
    "The SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5. We first load the data and have the shapes printed out. The split into train, validation and test set has already been carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X_train = np.load('sign_data/X_train.npy')\n",
    "Y_train = np.load('sign_data/Y_train.npy')\n",
    "X_val = np.load('sign_data/X_val.npy')\n",
    "Y_val = np.load('sign_data/Y_val.npy')\n",
    "X_test = np.load('sign_data/X_test.npy')\n",
    "Y_test = np.load('sign_data/Y_test.npy')\n",
    "\n",
    "# print the shape of the dataset\n",
    "print(\"X_train shape: \" + str(X_train.shape))\n",
    "print(\"Y_train shape: \" + str(Y_train.shape))\n",
    "print(\"X_val shape: \" + str(X_val.shape))\n",
    "print(\"Y_val shape: \" + str(Y_val.shape))\n",
    "print(\"X_test shape: \" + str(X_test.shape))\n",
    "print(\"Y_test shape: \" + str(Y_test.shape)+\"\\n\")\n",
    "print(\"classes: \" + str(np.unique(Y_train)))\n",
    "\n",
    "# check if classes are balanced\n",
    "print(\"Counts of classes in Y_train: \" + str(np.unique(Y_train, return_counts=True)[1]))\n",
    "print(\"Counts of classes in Y_val: \" + str(np.unique(Y_val, return_counts=True)[1]))\n",
    "print(\"Counts of classes in Y_test: \" + str(np.unique(Y_test, return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c7720",
   "metadata": {},
   "source": [
    "The classes are balanced so that accuracy is an appropriate measure for evaluating a classifier. We next visualize an instance of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33238cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 6, figsize=(20, 10))\n",
    "for i in range(6):\n",
    "    # get indices where the label is i\n",
    "    idx = np.where(Y_train == i)[0][0]\n",
    "    axs[i].imshow(X_train[idx])\n",
    "    axs[i].set_title(\"y = \" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9e1405",
   "metadata": {},
   "source": [
    "Pixels in each channel (RGB) of the images take values in the range \\[0, 255\\]. However, it is desirable to have absolute values in the range \\[0, 1\\] as input for neural network architectures to avoid exploding or vanishing gradient problems. Through the following cell, we apply a simple data scaling procedure: we divide the values of the pixels by 255.\n",
    "As an alternative, you can use the `StandardScaler()` function of the scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec5daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_val = X_val/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10616dce",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba05e5c0",
   "metadata": {},
   "source": [
    "Use pytorch to build the model. Take a look at the [documentation](https://pytorch.org/tutorials/beginner/basics/intro.html) for an introduction, a detailed tutorial, for example for classifiers, can be found [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html).\n",
    "\n",
    "Implement the following architecture:\n",
    "\n",
    "     - Conv2d: 4 output channels, 3 by 3 filter size, stride 1, padding \"same\"\n",
    "     - BatchNorm2d: 4 output channels\n",
    "     - ReLU activation\n",
    "     - MaxPool2d: 2 by 2 filter size, stride 2, padding 0\n",
    "     - Conv2d: 8 output channels, 3 by 3 filter size, stride 1, padding \"same\"\n",
    "     - BatchNorm2d: 8 output channels\n",
    "     - ReLU activation\n",
    "     - MaxPool2d: Use a 2 by 2 filter size, stride 2, padding 0\n",
    "     - Flatten the previous output\n",
    "     - Linear: 64 output neurons\n",
    "     - ReLu activation function\n",
    "     - Linear: 6 output neurons\n",
    "     - LogSoftmax\n",
    "\n",
    "We use the [LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) here instead of the Softmax for computational reasons. Accordingly, the loss function is not CrossEntropyLoss but NLLLoss. When flattening, be careful not to do it with the batch dimension but only with the height, width and channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430c072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Initialize the layers of the CNN\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # TODO: Implement the forward pass\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48675d25",
   "metadata": {},
   "source": [
    "To test your model you can foward some random numbers. The shape of the output should be (2, 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN_Classifier()\n",
    "# dummy sample of batch size 2\n",
    "X_random = tc.randn(2, 3, 64, 64)\n",
    "output = cnn_model(X_random)\n",
    "\n",
    "print(\"Output shape: \" + str(output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ecfb2",
   "metadata": {},
   "source": [
    "torchsummary.summary provides a nice overview of the model and the number of learnable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cnn_model, input_size=(3, 64, 64), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf52103",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8534b18",
   "metadata": {},
   "source": [
    "DataLoaders wrap around Datasets to provide efficient data batching, shuffling, and parallel loading during model training or inference. To define a custom dataset we must implement three functions: __init__, __len__ and __get_item__. While __len__ defines the length of the dataset and thus the number of batches in the dataloader, __get_item__ can be used to get a single sample through the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: Return the length of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Return the image as a float tensor and the label as a long tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da3dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "val_batch_size = len(Y_val)\n",
    "test_batch_size = len(Y_test)\n",
    "\n",
    "# TODO: Create the dataset and dataloader\n",
    "train_dataset = \n",
    "val_dataset = \n",
    "test_dataset = \n",
    "train_loader = \n",
    "val_loader = \n",
    "test_loader = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59640141",
   "metadata": {},
   "source": [
    "To make sure that everything has worked properly, we take a sample of the data_loader and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae114b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_X, sample_Y = next(iter(train_loader))\n",
    "plt.imshow(sample_X[0].T)\n",
    "plt.title(\"y = \" + str(int(sample_Y[0].item())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffdcdb1",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a8c0c2",
   "metadata": {},
   "source": [
    "Implement the training loop. Use the negative log-likelihood loss (NLLLoss) and the Adam optimizer. Be sure to zero the gradients after each optimization step to avoid accumulating contributions from previous epochs and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f652bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(model, train_loader, val_loader, lr, n_epochs, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # TODO: Initialize the optimizer and loss function\n",
    "    loss_function =  \n",
    "    optimizer = \n",
    "\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "    val_loss = np.zeros(n_epochs)\n",
    "    train_acc = np.zeros(n_epochs)\n",
    "    val_acc = np.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for X, Y in train_loader:\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            # TODO: Implement the training loop\n",
    "\n",
    "            epoch_loss += loss.item()/len(train_loader)\n",
    "        \n",
    "        train_loss[epoch - 1] = epoch_loss\n",
    "        train_acc[epoch - 1] = (output.argmax(dim=1) == Y).float().mean().item()\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        with tc.no_grad():\n",
    "            X, Y = next(iter(val_loader))\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            # TODO: Implement the evaluation step\n",
    "                  \n",
    "            val_loss[epoch - 1] = loss.item()\n",
    "            val_acc[epoch - 1] = (output.argmax(dim=1) == Y).float().mean().item()\n",
    "        print(f\"Epoch {epoch}/{n_epochs} - Train Loss: {epoch_loss:.4f}, Test Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return train_loss, val_loss, train_acc, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e0130",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "# TODO: Train the model with different learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30901fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b6e07",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ca3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: apply the best model to the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3906bd",
   "metadata": {},
   "source": [
    "## Exercise 3 -  CNN Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdee05",
   "metadata": {},
   "source": [
    "In the next task we want to build an autoencoder. It consists of an encoder, which transforms the data into a low-dimensional code, and a decoder, which reconstructs the original data. \n",
    "\n",
    "We use the Fashion MNIST dataset, which consists of 28x28 grayscale images. There are 10 classes, each representing different items of clothing. The data can be conveniently downloaded, separated and transformed with torchvision. As we will not be tuning any hyperparameters, we do not need a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL image to tensor (range [0, 1])\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Load FashionMNIST train and test sets\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root='./fashion_mnist',          # Download path\n",
    "    train=True,            # Load training set\n",
    "    download=True,         # Download if not already present\n",
    "    transform=transform    # Apply transformations\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='./fashion_mnist',\n",
    "    train=False,           # Load test set\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "classes = train_data.targets.unique()\n",
    "\n",
    "# we only need a subset that consists of 1000 samples of each class for the train set\n",
    "# and 10 samples of each class for the test set\n",
    "indices_train = []\n",
    "indices_test = []\n",
    "for i in range(len(classes)):\n",
    "    indices_train += list(np.where(train_data.targets == classes[i])[0][:1000])\n",
    "    indices_test += list(np.where(test_data.targets == classes[i])[0][:10])\n",
    "    \n",
    "train_data.data = train_data.data[indices_train]\n",
    "train_data.targets = train_data.targets[indices_train]\n",
    "test_data.data = test_data.data[indices_test]\n",
    "test_data.targets = test_data.targets[indices_test]\n",
    "\n",
    "# Create DataLoaders \n",
    "train_batch_size = 256\n",
    "test_batch_size = len(test_data) \n",
    "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(\"train images shape: \" + str(train_data.data.shape))\n",
    "print(\"train labels shape: \" + str(train_data.targets.shape))\n",
    "print(\"test images shape: \" + str(test_data.data.shape))\n",
    "print(\"test labels shape: \" + str(test_data.targets.shape))\n",
    "print(\"classes: \" + str(classes))\n",
    "\n",
    "# check if classes are balanced\n",
    "print(\"Counts of classes in train set: \" + str(train_data.targets.unique(return_counts=True)[1]))\n",
    "print(\"Counts of classes in test set: \" + str(test_data.targets.unique(return_counts=True)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87b0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Visualize a batch of images\n",
    "fig, axes = plt.subplots(4, 10, figsize=(12, 6))\n",
    "for i in range(4):\n",
    "    for j in range(10):\n",
    "        # find the ith image of class j\n",
    "        idx = np.where(labels == j)[0][i]\n",
    "        axes[i, j].imshow(images[idx].squeeze(), cmap='gray')\n",
    "        axes[i, j].set_title(f\"Label: {j}\")\n",
    "        axes[i, j].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7031bd",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc79bfe",
   "metadata": {},
   "source": [
    "We compare two architectures: a linear autoencoder and a CNN autoencoder. The latter typically consists of convolutional layers for the encoder and transposed convolutional layers for the decoder. In addition, fully connected layers can bring the feature to the desired code dimension (also called latent dimension). Implement the following architecture:\n",
    "\n",
    "Encoder:\n",
    "- Conv2d: 16 output channels, 3 by 3 filter size, stride 1, padding \"same\"\n",
    "- ReLU activation\n",
    "- MaxPool2d: 2 by 2 filter, stride 1, padding 0\n",
    "- Conv2d: 32 output channels, 3 by 3, stride 1, padding \"same\"\n",
    "- ReLU activation\n",
    "- MaxPool2d: 2 by 2 filter, stride 1, padding 0\n",
    "- Conv2d: 64 output channel, 3 by 3 filter size, stride 1, padding \"same\"\n",
    "- ReLU activation\n",
    "- MaxPool2d: 2 by 2 filter, stride 1, padding 0\n",
    "- Flatten the previous output\n",
    "- Linear: <_latent dimension_> output neurons\n",
    "\n",
    "Decoder:\n",
    "- Linear: 64x3x3 = 576 output neurons\n",
    "- Unflatten the previous output to shape (64, 3, 3)\n",
    "- ConvTranspose2d: 32 output channels, 3 by 3 filter size, stride 2, padding 0, output padding 1\n",
    "- ReLU activation\n",
    "- ConvTranspose2d: 16 output channels, 3 by 3 filter size, stride 2, padding 1, output padding 1\n",
    "- ReLU activation\n",
    "- ConvTranspose2d: 1 output channel, 3 by 3 filter size, stride 2, padding 1, output padding 1\n",
    "\n",
    "You might need to infer the input dimension of the linear layer in the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c63442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_AE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Conv_AE, self).__init__()\n",
    "\n",
    "        # TODO: Initialize the layers of the encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "        )\n",
    "        \n",
    "        # TODO: Initialize the layers of the decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "        )\n",
    "            \n",
    "    def forward(self, X):\n",
    "        # TODO: Implement the forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4def8f",
   "metadata": {},
   "source": [
    "We check again whether the model is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34979c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ae_model = Conv_AE(2)\n",
    "X_random = tc.randn(2, 1, 28, 28)\n",
    "reconstructed, latent = conv_ae_model(X_random)\n",
    "\n",
    "print(\"Reconstructed shape: \" + str(reconstructed.shape), \"\\n\", \"Latent shape: \" + str(latent.shape))\n",
    "\n",
    "summary(conv_ae_model, input_size=(1, 28, 28), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0bfb5",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5519c",
   "metadata": {},
   "source": [
    "The training of an autoencoder compares the original input to the reconstruction, usually by means of the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(model, train_loader, test_loader, lr, n_epochs, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # TODO: Initialize the loss function\n",
    "    loss_function = \n",
    "    optimizer = \n",
    "\n",
    "    train_loss = np.zeros(n_epochs)\n",
    "    test_loss = np.zeros(n_epochs)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for X, _ in train_loader:\n",
    "            X = X.to(device)\n",
    "            # TODO: Implement the training loop\n",
    "\n",
    "            epoch_loss += loss.item()/len(train_loader)\n",
    "        \n",
    "        train_loss[epoch - 1] = epoch_loss\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        with tc.no_grad():\n",
    "            X, _ = next(iter(test_loader))\n",
    "            X = X.to(device)\n",
    "            # TODO: Implement the evaluation step\n",
    "\n",
    "            \n",
    "            test_loss[epoch - 1] = loss.item()\n",
    "        print(f\"Epoch {epoch}/{n_epochs} - Train Loss: {epoch_loss:.4f}, Test Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f657ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 60\n",
    "lr = 1e-3\n",
    "# TODO: Train the convolutional autoencoder model with different latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a01df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Visualize the reconstructed images and the original images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2b0ed",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bc724",
   "metadata": {},
   "source": [
    "A linear autoencoder aims to represent the data $X\\in \\mathbb{R}^{n\\times m}$ in a new basis using only $d<m$ directions.\n",
    "The objective is to minimize the squared error between $X$ and $D(E(X))$ where $E: \\mathbb{R}^{m}\\to\\mathbb{R}^{d}$ is the encoder and $D: \\mathbb{R}^{d}\\to\\mathbb{R}^{m}$ is the decoder:\n",
    "$$\\|D(E(X)) - X\\|^2_2$$\n",
    "\n",
    "In geometric terms, we want to find $d$ axes along which most of the variance occurs which is exactly what Principal Component Analysis does. The optimal weights of a linear autoencoder with code dimension $d$ thus span the same space as the first $d$ principal components.\n",
    "\n",
    "The PCA autoencoder just consists of 1 linear layer for the encoder and 1 linear layer for the decoder. The bias is necessary to subtract the mean value. Since we are dealing with three-dimensional images, we also have to flatten (when encoding) or unflatten (when decoding) the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe79622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_AE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(PCA_AE, self).__init__()\n",
    "        \n",
    "        # TODO: Initialize the layers of the autoencoder\n",
    "        self.encoder = nn.Sequential()\n",
    "        \n",
    "        self.decoder = nn.Sequential()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO: Implement the forward call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc21af",
   "metadata": {},
   "source": [
    "For the training we can use the same function as for the convolutional autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8117912",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 60\n",
    "lr = 1e-3\n",
    "# TODO: Train the PCA autoencoder model with a latent dimension of 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90875b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the training and test loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the reconstructed images and the original images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90b8350",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Choose the convolutional autoencoder with latent dimension 3 and encode 800 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab4c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a 3D scatter plot of the code colored by the labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_essentials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
